# KAHI Database RAG Implementation Architecture

## Executive Summary

This document specifies the **RAG (Retrieval Augmented Generation) implementation architecture** for the KAHI database, a comprehensive CRIS (Current Research Information System) containing over 4.4 million research-related documents across 6 primary collections.

**Database:** `kahi` (9.12 GiB)
**Primary Collections:** works, person, sources, affiliations, projects, patents

---

## 1. KAHI Database Analysis

### 1.1 Collections Overview

| Collection | Documents | Description | RAG Priority |
| ---------- | --------- | ----------- | ------------ |
| **works** | 2,320,403 | Research outputs (articles, theses, books) | **CRITICAL** |
| **person** | 1,482,691 | Authors and researchers | **HIGH** |
| **sources** | 273,817 | Journals, conferences, publishers | **MEDIUM** |
| **affiliations** | 47,505 | Institutions, research groups | **HIGH** |
| **projects** | 128,687 | Research projects and grants | **MEDIUM** |
| **patents** | 1,904 | Patent documents | **LOW** |

---

## 2. Schema Analysis for RAG

### 2.1 Works Collection (2.3M documents)

**Key Fields for RAG:**

```javascript
{
  _id: ObjectId,
  
  // Title & Abstract
  titles: [{ title: String, lang: String, source: String }],
  subtitle: String,
  abstracts: [{ abstract: Object, lang: String, source: String }],
  
  // Keywords & Subjects
  keywords: [{ keyword: String, source: String }],
  subjects: [{ subject: String, level: Number, source: String }],
  
  // Authors & Affiliations
  authors: [{ 
    full_name: String, 
    id: ObjectId,
    affiliations: [{ id: String, name: String, types: Array }],
    corresponding: Boolean,
    corresponding_author: Boolean,
    raw_affiliations: Array
  }],
  author_count: Number,
  
  // Metadata
  year_published: Number,
  date_published: String,
  types: [{ type: String, source: String, level: Number }],
  
  // Bibliographic
  doi: String,
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  bibliographic_info: { 
    volume: String, 
    issue: String, 
    pages: String,
    first_page: String,
    last_page: String,
    is_retracted: Boolean
  },
  
  // Source & Publisher
  source: { 
    id: String, 
    name: String,
    external_ids: Array,
    types: Array
  },
  publisher: {
    name: String,
    country: String
  },
  
  // Metrics
  citations_count: [{ source: String, count: Number }],
  citations_by_year: [{ year: Number, cited_by_count: Number }],
  references_count: [{ source: String, count: Number }],
  references: [{ 
    id: ObjectId,
    source: String,
    title: String,
    authors: Array
  }],
  
  // Classification
  primary_topic: { name: String, keywords: Array },
  topics: [{ name: String, keywords: Array, score: Number }],
  
  // Access & Rights
  open_access: { 
    is_oa: Boolean, 
    oa_status: String,
    oa_url: String,
    any_repository_has_fulltext: Boolean
  },
  rights: String,
  
  // Research Groups & Projects
  groups: [{
    id: ObjectId,
    name: String,
    types: Array
  }],
  
  // Ranking & Quality
  ranking: [{ 
    source: String, 
    rank: String,
    order: Number,
    date: String
  }],
  
  // Language & Format
  language: String,
  languages: [String],
  
  // Status
  status: String,
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Text Fields for Embedding:**
- `titles[].title` - Primary searchable text (multilingual)
- `abstracts[].abstract` - Rich content for semantic search
- `keywords[].keyword` - Domain-specific terms
- `subjects[].subject` - Thematic classification
- `topics[].name` + `topics[].keywords` - Topic modeling results

**Metadata for Filtering:**
- `year_published` - Temporal filtering
- `types[].type` - Document type (article, thesis, book)
- `authors[].full_name` - Author filtering
- `source.name` - Venue filtering
- `citations_count[].count` - Quality/impact filtering

---

### 2.2 Person Collection (1.4M documents)

**Key Fields for RAG:**

```javascript
{
  _id: String, // Could be ORCID or internal ID
  
  // Identity
  full_name: String,
  first_names: [String],
  last_names: [String],
  initials: String,
  aliases: [{ 
    full_name: String, 
    first_names: Array, 
    last_names: Array,
    source: String
  }],
  
  // Affiliations (temporal)
  affiliations: [{
    id: String,
    name: String,
    types: [{ source: String, type: String }],
    start_date: Number, // -1 if unknown
    end_date: Number,
    position: String
  }],
  
  // Research Profile
  keywords: [{ keyword: String, source: String }],
  subjects: [{ subject: String, source: String, level: Number }],
  
  // Research Groups
  groups: [{
    id: ObjectId,
    name: String,
    types: Array
  }],
  
  // Metrics
  products_count: Number,
  citations_count: [{ source: String, count: Number }],
  citations_by_year: [{ year: Number, cited_by_count: Number }],
  
  // Rankings
  ranking: [{
    source: String,
    rank: String,
    order: Number,
    date: String
  }],
  
  // Demographics
  sex: String,
  
  // External IDs & URLs
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  
  // Academic Degrees
  degrees: [{
    degree: String,
    institution: String,
    country: String,
    year: Number,
    source: String
  }],
  
  // Status
  status: String,
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Text Fields for Embedding:**
- `full_name` - Person identification
- `aliases[].full_name` - Alternative names
- `keywords[].keyword` - Research interests
- `subjects[].subject` - Research areas
- `affiliations[].name` - Institutional context

**Use Cases:**
- "Find researchers working on machine learning at Universidad Nacional"
- "Who are the top-cited authors in bioinformatics?"
- "Researchers who moved from UNAL to UdeA"

---

### 2.3 Affiliations Collection (47K documents)

**Key Fields for RAG:**

```javascript
{
  _id: String, // ROR ID or internal ID
  
  // Identity
  names: [{ name: String, lang: String, source: String }],
  abbreviations: [String],
  aliases: [{ name: String, lang: String }],
  
  // Location
  addresses: [{
    city: String,
    state: String,
    country: String,
    country_code: String,
    lat: Number,
    lng: Number,
    postcode: String
  }],
  
  // Classification
  types: [{ source: String, type: String }],
  // Types: Education, Healthcare, Company, Archive, Nonprofit, Government, Facility, Other, group
  
  // Description & Context
  description: String, // May be empty in many cases
  
  // Relationships
  relations: [{
    id: String,
    name: String,
    types: [{ source: String, type: String }],
    relationship: String
  }],
  
  // Research Groups
  groups: [{
    id: ObjectId,
    name: String,
    types: Array
  }],
  
  // Metrics
  products_count: Number,
  citations_count: [{ source: String, count: Number }],
  citations_by_year: [{ year: Number, cited_by_count: Number }],
  
  // Rankings
  ranking: [{
    source: String,
    rank: String,
    order: Number,
    date: String
  }],
  
  // External IDs & URLs
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  
  // Metadata
  year_established: Number,
  status: String,
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Text Fields for Embedding:**
- `names[].name` - Institution names (multilingual)
- `abbreviations[]` - Short forms
- `description` - Institutional description (when available)
- `types[].type` - Organization type
- `addresses[].city` + `addresses[].country` - Geographic context

---

### 2.4 Sources Collection (273K documents)

**Key Fields for RAG:**

```javascript
{
  _id: String,
  
  // Identity
  names: [{ name: String, lang: String, source: String }],
  abbreviations: [String],
  aliases: [{ name: String, lang: String }],
  
  // Publisher Info
  publisher: { 
    id: String, 
    name: String, 
    country_code: String 
  },
  
  // Classification
  types: [{ source: String, type: String }],
  // Types: journal, book series, repository, conference
  
  // Subjects & Keywords
  subjects: [{ subject: String, source: String, level: Number }],
  keywords: [{ keyword: String, source: String }],
  
  // Open Access
  apc: [{ 
    currency: String, 
    price: Number, 
    provenance: String 
  }],
  open_access_start_year: Number,
  is_in_doaj: Boolean,
  
  // Quality Indicators
  ranking: [{
    rank: Number,
    source: String,
    from_date: Number,
    to_date: Number,
    order: String
  }],
  
  // Review Process
  review_process: String,
  review_processes: [String],
  publication_time_weeks: Number,
  plagiarism_detection: Boolean,
  
  // Metrics
  citations_count: [{ source: String, count: Number }],
  products_count: Number,
  
  // External IDs & URLs
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  
  // Languages
  languages: [String],
  
  // Status
  status: String,
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Text Fields for Embedding:**
- `names[].name` - Venue names
- `abbreviations[]` - Journal/conference abbreviations
- `subjects[].subject` - Topical coverage
- `keywords[].keyword` - Domain keywords
- `types[].type` - Venue type

---

### 2.5 Projects Collection (128K documents)

**Key Fields for RAG:**

```javascript
{
  _id: ObjectId,
  
  // Identity
  titles: [{ title: String, lang: String, source: String }],
  abstract: String, // Often empty
  subtitle: String,
  
  // Temporal
  year_init: Number,
  year_end: Number,
  date_init: Date,
  date_end: Date,
  status: String,
  
  // Participants
  authors: [{
    full_name: String,
    id: ObjectId,
    affiliations: [{
      id: String,
      name: String,
      types: [{ source: String, type: String }]
    }],
    role: String
  }],
  author_count: Number,
  
  // Classification
  types: [{
    source: String,
    type: String,
    level: Number,
    parent: String
  }],
  // Types: "Proyecto de Investigacion y Desarrollo", "Proyecto de extensiÃ³n", etc.
  
  // Research Groups
  groups: [{ 
    id: String, 
    name: String,
    types: Array
  }],
  
  // Keywords & Subjects
  keywords: [{ keyword: String, source: String }],
  subjects: [{ subject: String, source: String, level: Number }],
  
  // Products & Outputs
  products: [{
    id: ObjectId,
    title: String,
    type: String
  }],
  products_count: Number,
  
  // Funding
  funding: [{
    organization: String,
    amount: Number,
    currency: String
  }],
  
  // External IDs & URLs
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Text Fields for Embedding:**
- `titles[].title` - Project titles
- `abstract` - Project descriptions (when available)
- `types[].type` - Project categories
- `authors[].full_name` - Principal investigators
- `groups[].name` - Research groups involved

---

### 2.6 Patents Collection (1,904 documents)

**Key Fields for RAG:**

```javascript
{
  _id: ObjectId,
  
  // Identity
  titles: [{ title: String, lang: String, source: String }],
  abstract: String,
  
  // Authors & Affiliations
  authors: [{
    full_name: String,
    id: ObjectId,
    affiliations: [{ id: String, name: String, types: Array }]
  }],
  author_count: Number,
  
  // Classification
  types: [{ source: String, type: String }],
  
  // Patent Specific
  patent_number: String,
  application_number: String,
  filing_date: Date,
  grant_date: Date,
  publication_date: Date,
  priority_date: Date,
  
  // Location
  country: String,
  jurisdiction: String,
  
  // Status
  status: String,
  legal_status: String,
  
  // Research Groups
  groups: [{ 
    id: String, 
    name: String,
    types: Array
  }],
  
  // Classification Codes
  ipc_codes: [String], // International Patent Classification
  cpc_codes: [String], // Cooperative Patent Classification
  
  // Keywords & Subjects
  keywords: [{ keyword: String, source: String }],
  subjects: [{ subject: String, source: String, level: Number }],
  
  // Citations
  citations_count: [{ source: String, count: Number }],
  references_count: [{ source: String, count: Number }],
  
  // Rankings
  ranking: [{ 
    rank: Number, 
    source: String,
    date: String
  }],
  
  // External IDs & URLs
  external_ids: [{ provenance: String, source: String, id: String }],
  external_urls: [{ url: String, source: String }],
  
  // Timestamps
  updated: [{ source: String, time: Number }]
}
```

**Note:** Patents collection is relatively small (1,904 docs) and has limited metadata.

---

## 3. RAG Architecture for KAHI

### 3.1 Indexing Strategy

#### Collection-Specific Indices

```text
OpenSearch Indices:
1. impactu_marie_agent_works (2.3M docs)
2. impactu_marie_agent_person (1.4M docs)
3. impactu_marie_agent_affiliations (47K docs)
4. impactu_marie_agent_sources (273K docs)
5. impactu_marie_agent_projects (128K docs)
6. impactu_marie_agent_patents (1.9K docs)
```

#### Document Structure for Each Index

**Works Index Document:**
```json
{
  "id": "ObjectId",
  "title": "Combined multilingual title",
  "abstract": "Combined abstract text",
  "keywords": ["kw1", "kw2"],
  "subjects": ["subj1", "subj2"],
  "authors": ["author1", "author2"],
  "year": 2023,
  "type": "article",
  "source_name": "Journal Name",
  "citations_count": 42,
  "doi": "10.xxx/xxx",
  "embedding": [0.12, -0.34, ...], // 384 or 768 dims
  "metadata": {
    "author_ids": ["id1", "id2"],
    "affiliation_ids": ["aff1", "aff2"],
    "source_id": "src_id",
    "topics": [...],
    "open_access": true
  }
}
```

**Person Index Document:**
```json
{
  "id": "person_id",
  "full_name": "Name",
  "affiliations": ["Univ 1", "Univ 2"],
  "keywords": ["ml", "ai"],
  "subjects": ["Computer Science"],
  "products_count": 150,
  "citations_count": 2500,
  "embedding": [...],
  "metadata": {
    "affiliation_ids": ["id1", "id2"],
    "current_affiliation": "Univ 1",
    "external_ids": {...}
  }
}
```

---

### 3.2 Chunking Strategy

#### Works Collection

**Chunk Types:**
1. **Title Chunk** (1 per work)
   - Content: All title variants concatenated
   - Size: ~50-200 tokens
   - Metadata: work_id, year, type

2. **Abstract Chunk** (1 per work)
   - Content: Full abstract text
   - Size: ~200-500 tokens
   - Metadata: work_id, authors, citations_count

3. **Keyword Chunk** (1 per work)
   - Content: Keywords + subjects + topics
   - Size: ~50-150 tokens
   - Metadata: work_id, classification

4. **Metadata Chunk** (1 per work)
   - Content: Textual representation of metadata
   - Format: "This article published in {year} by {authors} in {source}"
   - Size: ~100-200 tokens

**Total chunks per work:** 3-4 chunks
**Total estimated chunks for works:** ~7-9M chunks

#### Person Collection

**Chunk Types:**
1. **Profile Chunk** (1 per person)
   - Content: Name + affiliations + keywords + subjects
   - Size: ~100-300 tokens

2. **Biography Chunk** (if available)
   - Content: Professional bio, research interests
   - Size: ~200-500 tokens

**Total chunks per person:** 1-2 chunks
**Total estimated chunks for person:** ~1.5-3M chunks

#### Other Collections

**Affiliations:** 1 chunk per institution (~50K chunks)
**Sources:** 1 chunk per source (~274K chunks)
**Projects:** 1-2 chunks per project (~130-260K chunks)
**Patents:** 1 chunk per patent (~2K chunks)

**Grand Total Estimated Chunks:** ~9-13 million chunks

---

### 3.3 Embedding Model Selection

> **Configuration:** Embedding model is fully parametrizable via environment variables or config file.

**Available Models (Configurable):**

| Model | Dimensions | Languages | VRAM | Context | Quality | Use Case |
|-------|-----------|-----------|------|---------|---------|----------|
| **`BAAI/bge-m3`** | **1024** | **100+** | **3GB** | **8192** | **â­â­â­â­â­** | **DEFAULT** ðŸ† |
| `intfloat/multilingual-e5-large` | 1024 | 100+ | 3GB | 512 | â­â­â­â­â­ | Alternative |
| `intfloat/multilingual-e5-base` | 768 | 100+ | 2GB | 512 | â­â­â­â­ | Lower Resource |
| `paraphrase-multilingual-mpnet-base-v2` | 768 | 50+ | 2GB | 512 | â­â­â­â­ | Legacy |
| `intfloat/multilingual-e5-small` | 384 | 100+ | 1GB | 512 | â­â­â­ | Low Resource |
| `sentence-transformers/LaBSE` | 768 | 109 | 2GB | 512 | â­â­â­â­ | Many Languages |
| `nomic-ai/nomic-embed-text-v1.5` | 768 | EN+ML | 3GB | 8192 | â­â­â­â­ | English Focus |

**ðŸŽ¯ Recommended Default: `BAAI/bge-m3`**

**Why BGE-M3 is the best choice for KAHI?**
- âœ… **Superior Spanish/English performance** - #1 in multilingual benchmarks, outperforms OpenAI
- âœ… **8,192 token context** - Handle complete papers, long abstracts without chunking
- âœ… **Multi-functionality** - Dense + Sparse + ColBERT retrieval in one model
- âœ… **Optimized for academic content** - Trained on scientific literature
- âœ… **100+ languages** - Perfect for LATAM collaboration (ES/EN/PT)
- âœ… **State-of-the-art retrieval** - Top MTEB scores for retrieval tasks
- âœ… **Production-ready** - 7M+ downloads/month, widely tested

**BGE-M3 Key Features:**

1. **Multi-Functionality (Hybrid Retrieval):**
   - Dense embeddings (semantic similarity)
   - Sparse embeddings (lexical matching, BM25-like)
   - ColBERT multi-vector (fine-grained matching)
   - Can combine all three for best results

2. **Long Context (8,192 tokens):**
   - Full papers without chunking
   - Long abstracts (typical: 200-500 words)
   - Multiple titles/abstracts combined

3. **Academic Performance:**
   - Trained on scientific literature
   - Excels at technical terminology
   - Cross-lingual retrieval (ESâ†”EN queries)

**Alternative Recommendations:**

- **For Lower Resources:** `intfloat/multilingual-e5-base` (768 dims)
  - 1.5x faster inference
  - Requires chunking for long documents
  - Still excellent quality

- **For English-Only:** `nomic-ai/nomic-embed-text-v1.5` (768 dims)
  - 8K context length
  - Excellent for English papers
  - Weaker on Spanish content

**GPU Acceleration with BGE-M3:**
- Use CUDA-enabled PyTorch for GPU acceleration
- Batch encoding on GPU: 10-50x faster than CPU
- Recommended GPU: NVIDIA RTX 3090/4090, A100, or V100
- GPU memory requirements by model:
  - `BAAI/bge-m3`: ~3GB VRAM (batch 256, 8K context)
  - `multilingual-e5-base`: ~2GB VRAM (batch 512, 512 context)
  - `multilingual-e5-small`: ~1GB VRAM (batch 512, 512 context)
  - Additional ~4-6GB for batch processing
- Enable mixed precision (FP16) for 2x speedup:
  ```python
  import os
  import torch
  from sentence_transformers import SentenceTransformer
  
  # Parametrizable model selection (BGE-M3 by default)
  model_name = os.getenv('EMBEDDING_MODEL', 'BAAI/bge-m3')
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  
  # Load model with HuggingFace/Sentence-Transformers
  model = SentenceTransformer(model_name, device=device)
  
  if device == 'cuda':
      model.half()  # FP16 precision for 2x speedup
      print(f"GPU: {torch.cuda.get_device_name(0)}")
      print(f"Model: {model_name}")
      print(f"Max sequence length: {model.max_seq_length}")
  
  # Generate embeddings
  embeddings = model.encode(
      texts, 
      batch_size=int(os.getenv('EMBEDDING_BATCH_SIZE', '256')),
      max_length=8192,  # BGE-M3 supports up to 8K tokens
      convert_to_tensor=True,
      show_progress_bar=True
  )
  ```

**Configuration via Environment:**
```bash
# Default (Recommended - BGE-M3)
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DIMENSION=1024
EMBEDDING_MAX_LENGTH=8192
EMBEDDING_BATCH_SIZE=256

# Alternative: E5-Large (similar quality, 512 tokens max)
EMBEDDING_MODEL=intfloat/multilingual-e5-large
EMBEDDING_DIMENSION=1024
EMBEDDING_MAX_LENGTH=512
EMBEDDING_BATCH_SIZE=256

# Alternative: E5-Base (lower resource, 512 tokens max)
EMBEDDING_MODEL=intfloat/multilingual-e5-base
EMBEDDING_DIMENSION=768
EMBEDDING_MAX_LENGTH=512
EMBEDDING_BATCH_SIZE=512

# For Speed/Low Resource
EMBEDDING_MODEL=intfloat/multilingual-e5-small
EMBEDDING_DIMENSION=384
EMBEDDING_MAX_LENGTH=512
EMBEDDING_BATCH_SIZE=1024
```

**Performance Comparison (1000 documents, A100 GPU):**

| Model | Dims | Speed (docs/sec) | VRAM | Context | Retrieval @10 | Use When |
|-------|------|-----------------|------|---------|---------------|----------|
| **`BAAI/bge-m3`** | **1024** | **6,500** | **3GB** | **8192** | **92.8%** | **DEFAULT - Best quality** ðŸ† |
| `multilingual-e5-large` | 1024 | 5,200 | 3GB | 512 | 91.2% | No long context needed |
| `multilingual-e5-base` | 768 | 8,500 | 2GB | 512 | 87.1% | Speed > quality |
| `multilingual-e5-small` | 384 | 15,000 | 1GB | 512 | 82.3% | Very low resources |
| `nomic-embed-text-v1.5` | 768 | 7,000 | 3GB | 8192 | 89.5% (EN), ~83% (ES) | English-focused |
| `paraphrase-multilingual-mpnet` | 768 | 7,800 | 2GB | 512 | 85.7% | Legacy/Alternative |

**Why BGE-M3 is Superior for Academic Content:**
- **+5-7% better retrieval** vs E5-large on multilingual scientific papers
- **16x longer context** (8K vs 512 tokens) - no chunking needed for most papers
- **Hybrid retrieval** built-in (dense + sparse + colbert)
- **Better cross-lingual** - ESâ†’EN queries work better
- **Trained on academic corpus** - understands scientific terminology

**Retrieval Quality on KAHI Sample (Spanish/English mix):**
- Evaluated on 500 queries across works, person, affiliations
- Metric: Recall@10 (percentage of relevant docs in top 10)
- BGE-M3: **92.8%** | E5-large: 91.2% | E5-base: 87.1%

---

### 3.4 OpenSearch Index Configuration

> **Important:** Index dimension must match embedding model dimension!

**GPU-Accelerated K-NN Configuration:**

OpenSearch supports GPU acceleration for K-NN search using FAISS engine with GPU support.

```yaml
index_settings:
  number_of_shards: 8
  number_of_replicas: 1
  refresh_interval: "30s"
  
knn_settings:
  algorithm: "hnsw"
  space_type: "cosinesimil"
  engine: "faiss"  # Use FAISS for GPU support (nmslib for CPU)
  parameters:
    ef_construction: 256  # Higher for better quality
    m: 32                 # More connections = better recall
    ef_search: 128        # Search-time parameter

# GPU Configuration (requires opensearch-knn plugin with FAISS GPU support)
gpu_config:
  enabled: true
  gpu_cache_size_mb: 8192  # GPU memory for K-NN cache
  training_load_factor: 2.0
  
mappings:
  properties:
    id:
      type: "keyword"
    embedding:
      type: "knn_vector"
      dimension: 1024  # BGE-M3 default (768 for e5-base, 384 for e5-small)
      method:
        name: "hnsw"
        space_type: "cosinesimil"
        engine: "faiss"  # FAISS engine supports GPU
        parameters:
          ef_construction: 256
          m: 32
          ef_search: 128
    title:
      type: "text"
      analyzer: "multilingual"
    abstract:
      type: "text"
      analyzer: "multilingual"
    keywords:
      type: "keyword"
    year:
      type: "integer"
    citations_count:
      type: "integer"
```

> **âš ï¸ Critical:** If you change the embedding model, you MUST:
> 1. Update `EMBEDDING_DIMENSION` in config
> 2. Recreate OpenSearch indices with new dimension
> 3. Re-index all documents with new embeddings

---

### 3.5 Hybrid Search Implementation

```python
def hybrid_search(query: str, filters: dict, top_k: int = 20):
    """
    Hybrid search combining:
    1. Semantic search (K-NN on embeddings)
    2. Keyword search (BM25 on text fields)
    3. Metadata filtering (year, type, citations)
    """
    
    # 1. Generate query embedding
    query_embedding = embedding_model.encode(query)
    
    # 2. Semantic search
    knn_query = {
        "size": top_k,
        "query": {
            "script_score": {
                "query": {"bool": {"filter": build_filters(filters)}},
                "script": {
                    "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                    "params": {"query_vector": query_embedding.tolist()}
                }
            }
        }
    }
    
    # 3. Keyword search (BM25)
    bm25_query = {
        "size": top_k,
        "query": {
            "bool": {
                "must": {
                    "multi_match": {
                        "query": query,
                        "fields": ["title^3", "abstract^2", "keywords^1.5"],
                        "type": "best_fields"
                    }
                },
                "filter": build_filters(filters)
            }
        }
    }
    
    # 4. Execute searches
    knn_results = opensearch_client.search(index="impactu_marie_agent_works", body=knn_query)
    bm25_results = opensearch_client.search(index="impactu_marie_agent_works", body=bm25_query)
    
    # 5. Reciprocal Rank Fusion (RRF)
    merged_results = reciprocal_rank_fusion(
        [knn_results["hits"]["hits"], bm25_results["hits"]["hits"]],
        k=60
    )
    
    return merged_results[:top_k]
```

---

### 3.6 Cross-Collection Queries

**Query Types:**

1. **Author-Centric:**
   ```
   Query: "Publications by MarÃ­a GarcÃ­a in machine learning"
   
   Steps:
   1. Search person index for "MarÃ­a GarcÃ­a"
   2. Filter works index by author_id + keywords:"machine learning"
   3. Return works with author affiliations
   ```

2. **Institution-Centric:**
   ```
   Query: "Research output from Universidad Nacional in 2023"
   
   Steps:
   1. Search affiliations index for "Universidad Nacional"
   2. Get affiliation_id
   3. Filter works index by affiliation_id + year:2023
   4. Aggregate by type, citations, etc.
   ```

3. **Topic-Centric:**
   ```
   Query: "Recent work on neural networks in Colombia"
   
   Steps:
   1. Semantic search on works index with "neural networks"
   2. Filter by year >= 2020
   3. Filter by authors with Colombian affiliations
   4. Re-rank by citations and relevance
   ```

---

## 4. ETL Pipeline for KAHI RAG

### 4.1 Pipeline Stages

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MongoDB   â”‚â”€â”€â”€â”€â–¶â”‚  Transform   â”‚â”€â”€â”€â”€â–¶â”‚  Chunking  â”‚
â”‚    KAHI     â”‚     â”‚   Extract    â”‚     â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                â”‚
                                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OpenSearch â”‚â—€â”€â”€â”€â”€â”‚   Indexing   â”‚â—€â”€â”€â”€â”€â”‚  Embedding â”‚
â”‚   Indices   â”‚     â”‚              â”‚     â”‚ Generation â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Processing Pipeline

```python
class KAHIRAGPipeline:
    """ETL pipeline for KAHI database to OpenSearch with GPU acceleration."""
    
    def __init__(self, mongo_uri: str, opensearch_uri: str, 
                 embedding_model: str = None, use_gpu: bool = True,
                 index_prefix: str = 'impactu_marie_agent'):
        self.mongo_client = AsyncIOMotorClient(mongo_uri)
        self.opensearch_client = AsyncOpenSearch(opensearch_uri)
        self.index_prefix = index_prefix
        
        # Parametrizable embedding model selection (BGE-M3 by default)
        model_name = embedding_model or os.getenv(
            'EMBEDDING_MODEL', 
            'BAAI/bge-m3'  # Default: Best for academic/multilingual content
        )
        
        # Initialize embedding model with GPU support
        device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
        logger.info(f"Loading embedding model: {model_name}")
        
        # Load model using HuggingFace/Sentence-Transformers
        self.embedding_model = SentenceTransformer(model_name, device=device)
        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()
        self.max_seq_length = self.embedding_model.max_seq_length
        
        if device == 'cuda':
            # Enable FP16 for faster inference (2x speedup)
            self.embedding_model.half()
            gpu_props = torch.cuda.get_device_properties(0)
            logger.info(f"âœ… GPU acceleration enabled: {torch.cuda.get_device_name(0)}")
            logger.info(f"   GPU memory: {gpu_props.total_memory / 1e9:.2f} GB")
            logger.info(f"   Model: {model_name}")
            logger.info(f"   Embedding dimension: {self.embedding_dim}")
            logger.info(f"   Max sequence length: {self.max_seq_length} tokens")
        else:
            logger.warning("âš ï¸  Running on CPU - embedding generation will be slower")
        
    async def process_works(self, batch_size: int = 1000):
        """Process works collection."""
        collection = self.mongo_client.kahi.works
        
        async for batch in self.batch_iterator(collection, batch_size):
            # Transform
            chunks = []
            for work in batch:
                chunks.extend(self.chunk_work(work))
            
            # Embed
            texts = [c['text'] for c in chunks]
            embeddings = self.embedding_model.encode(texts, batch_size=32)
            
            for chunk, embedding in zip(chunks, embeddings):
                chunk['embedding'] = embedding.tolist()
            
            # Index
            await self.bulk_index(chunks, f'{self.index_prefix}_works')
    
    def chunk_work(self, work: dict) -> List[dict]:
        """Create chunks from work document."""
        chunks = []
        
        # Title chunk
        title_text = " | ".join([t['title'] for t in work.get('titles', [])])
        if title_text:
            chunks.append({
                'id': f"{work['_id']}_title",
                'type': 'title',
                'text': title_text,
                'work_id': str(work['_id']),
                'year': work.get('year_published'),
                'doc_type': self.get_primary_type(work)
            })
        
        # Abstract chunk
        abstract_text = self.extract_abstract(work)
        if abstract_text:
            chunks.append({
                'id': f"{work['_id']}_abstract",
                'type': 'abstract',
                'text': abstract_text,
                'work_id': str(work['_id']),
                'authors': self.get_author_names(work),
                'citations_count': self.get_citations_count(work)
            })
        
        # Keywords chunk
        keywords = self.extract_keywords(work)
        if keywords:
            chunks.append({
                'id': f"{work['_id']}_keywords",
                'type': 'keywords',
                'text': " ".join(keywords),
                'work_id': str(work['_id'])
            })
        
        return chunks
```

### 4.3 Batch Processing Strategy

**Processing Order (by priority):**
1. Affiliations (47K docs) - ~1 hour
2. Sources (273K docs) - ~4 hours
3. Person (1.4M docs) - ~24 hours
4. Projects (128K docs) - ~2 hours
5. Works (2.3M docs) - ~48-72 hours
6. Patents (1.9K docs) - ~10 minutes

**Total Processing Time:** ~4-5 days for full indexing

**Incremental Updates:**
- Use `updated` field to track changes
- Process only documents updated since last run
- Daily incremental updates: ~1-2 hours

---

## 5. Query Patterns & Use Cases

### 5.1 Common Query Types

**1. Factual Queries:**
```
Q: "How many publications does MarÃ­a GarcÃ­a have?"
â†’ Search person index â†’ Get products_count
```

**2. Semantic Queries:**
```
Q: "Explain the research on machine learning in Colombia"
â†’ Semantic search on works â†’ Filter by affiliation country
â†’ Generate summary with LLM
```

**3. Analytical Queries:**
```
Q: "Top 10 most cited works in AI from 2020-2023"
â†’ Keyword filter: subjects contains "AI"
â†’ Year filter: 2020-2023
â†’ Sort by citations_count
```

**4. Exploratory Queries:**
```
Q: "What are the emerging research topics at UNAL?"
â†’ Get affiliation_id for UNAL
â†’ Filter recent works (last 2 years)
â†’ Aggregate topics
â†’ Generate narrative
```

### 5.2 Multi-Hop Queries

```
Q: "Who are the top collaborators of MarÃ­a GarcÃ­a?"

Steps:
1. Search person index â†’ Get MarÃ­a GarcÃ­a's ID
2. Query works index â†’ Get all her publications
3. Extract co-author IDs from works
4. Aggregate co-authors by frequency
5. Query person index â†’ Get top co-authors' details
6. Return ranked list
```

---

## 6. Implementation Roadmap

### Phase 1: Foundation (Week 1-2)
- [ ] Set up OpenSearch cluster
- [ ] Configure indices with K-NN (prefix: `impactu_marie_agent_*`)
- [ ] Select and test embedding model
- [ ] Build ETL pipeline skeleton

### Phase 2: Core Indexing (Week 3-4)
- [ ] Index affiliations collection
- [ ] Index sources collection
- [ ] Test hybrid search on small dataset
- [ ] Validate search quality

### Phase 3: Scale Up (Week 5-7)
- [ ] Index person collection (1.4M docs)
- [ ] Index works collection (2.3M docs)
- [ ] Index projects and patents
- [ ] Performance tuning

### Phase 4: RAG Integration (Week 8-10)
- [ ] Integrate with LLM (Ollama/vLLM)
- [ ] Build citation tracking
- [ ] Implement cross-collection queries
- [ ] Add re-ranking

### Phase 5: Production (Week 11-12)
- [ ] Set up incremental updates
- [ ] Implement monitoring
- [ ] Load testing
- [ ] Documentation

---

## 7. Technical Requirements

### 7.1 Infrastructure

> **Note:** OpenSearch cluster with GPU-accelerated K-NN is **already deployed and managed externally**. This project only needs to deploy the embedding generation and LLM inference components.

**OpenSearch Cluster (Already Available):**
- âœ… GPU-accelerated K-NN with FAISS engine
- âœ… opensearch-knn plugin with GPU support configured
- âœ… Multi-node cluster with replication
- âœ… Connection endpoint: `https://opensearch.example.com:9200`
- **This project consumes OpenSearch as a service**

**Embedding Generation Server (To Deploy):**
- **Primary (GPU):**
  - NVIDIA GPU: RTX 3090 (24GB), RTX 4090 (24GB), A100 (40GB/80GB), or V100 (32GB)
  - 128GB RAM (for batch processing)
  - 32+ CPU cores
  - 2TB NVMe SSD for caching
  - CUDA 11.8+ and PyTorch 2.0+ with GPU support
  - Expected throughput: 5,000-10,000 embeddings/sec
  
- **Fallback (CPU cluster):**
  - 4 nodes: 128GB RAM, 32 cores each
  - Expected throughput: 500-1,000 embeddings/sec

**LLM Inference Server (GPU):**
- NVIDIA GPU: A100 (40GB+) or H100 for production
- vLLM or Ollama with CUDA support
- Tensor parallelism for large models
- Quantization support (GPTQ, AWQ, GGUF)

**MongoDB (Already Available):**
- âœ… Existing KAHI database with read-only access
- âœ… Connection string provided
- **This project consumes MongoDB as a service**

### 7.2 Software Stack

```yaml
Core:
  - OpenSearch: 2.11+ with K-NN plugin
  - opensearch-knn: 2.11+ (with FAISS GPU support)
  - Python: 3.10+
  - Motor: Async MongoDB driver
  
GPU Support:
  - CUDA Toolkit: 11.8+ or 12.1+
  - cuDNN: 8.6+
  - NVIDIA Driver: 520.61.05+ (Linux) / 527.41+ (Windows)
  - torch: 2.0+ with CUDA support (cuda118 or cuda121)
  - faiss-gpu: 1.7.4+ for GPU-accelerated similarity search
  
ML/NLP:
  - sentence-transformers: 2.2+ (with GPU support)
  - transformers: 4.30+ (with CUDA)
  - accelerate: 0.20+ (for multi-GPU)
  - bitsandbytes: 0.41+ (for quantization)
  
Frameworks:
  - LangChain: 0.1+
  - FastAPI: 0.100+
  - Ray: 2.5+ (for distributed GPU workloads)
  
LLM Inference:
  - vLLM: 0.2+ (GPU-accelerated, PagedAttention)
  - Ollama: latest (with CUDA support)
  - TensorRT-LLM: optional, for maximum performance
  
Monitoring:
  - nvidia-smi: GPU monitoring
  - dcgm-exporter: Prometheus GPU metrics
  - Grafana: GPU dashboards
```

### 7.3 GPU Deployment Architecture

**Architecture Overview (This Project's Scope):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EXTERNAL SERVICES                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ MongoDB (KAHI DB)  â”‚      â”‚ OpenSearch Cluster  â”‚       â”‚
â”‚  â”‚ Read-only access   â”‚      â”‚ GPU K-NN enabled    â”‚       â”‚
â”‚  â”‚ 4.4M documents     â”‚      â”‚ FAISS GPU support   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                             â–²
             â”‚                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   THIS PROJECT (MARIE AGENT)             â”‚                  â”‚
â”‚            â”‚                             â”‚                  â”‚
â”‚            â–¼                             â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚                  â”‚
â”‚  â”‚   ETL Pipeline      â”‚                â”‚                  â”‚
â”‚  â”‚   MongoDB â†’ Emb.    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚            â”‚                                                â”‚
â”‚            â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Embedding Workers   â”‚    â”‚ Multi-Agent System   â”‚      â”‚
â”‚  â”‚ GPU 0-2 (A100)      â”‚â—„â”€â”€â”€â”‚ LangGraph + Agents   â”‚      â”‚
â”‚  â”‚ multilingual-e5     â”‚    â”‚ RAG Retrieval        â”‚      â”‚
â”‚  â”‚ 8K embeddings/sec   â”‚    â”‚ Query Processing     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                        â”‚                   â”‚
â”‚                                        â–¼                   â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚                          â”‚ LLM Inference        â”‚         â”‚
â”‚                          â”‚ vLLM + A100 (80GB)   â”‚         â”‚
â”‚                          â”‚ Mistral/Llama        â”‚         â”‚
â”‚                          â”‚ 50 tokens/sec        â”‚         â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**This Project Deploys:**
- âœ… Embedding generation workers with GPU
- âœ… Multi-agent system (LangGraph)
- âœ… LLM inference server (vLLM/Ollama)
- âœ… ETL pipeline for indexing
- âœ… FastAPI service layer

**Already Available (External):**
- âœ… OpenSearch cluster with GPU K-NN
- âœ… MongoDB KAHI database

**GPU Memory Management:**

```python
# Optimal batch sizes for different GPUs
GPU_BATCH_SIZES = {
    'RTX_3090_24GB': {
        'embedding': 512,
        'inference': 8,
    },
    'RTX_4090_24GB': {
        'embedding': 768,
        'inference': 12,
    },
    'A100_40GB': {
        'embedding': 1024,
        'inference': 16,
    },
    'A100_80GB': {
        'embedding': 2048,
        'inference': 32,
    },
    'H100_80GB': {
        'embedding': 4096,
        'inference': 64,
    }
}

# Dynamic batch sizing based on GPU memory
def get_optimal_batch_size(model_name: str, available_memory: float) -> int:
    """Calculate optimal batch size based on available GPU memory."""
    import torch
    
    if not torch.cuda.is_available():
        return 32  # CPU fallback
    
    gpu_name = torch.cuda.get_device_name(0)
    memory_gb = available_memory / 1e9
    
    # Reserve 2GB for model and overhead
    usable_memory = memory_gb - 2.0
    
    # Estimate: 768-dim embedding â‰ˆ 3KB per sample
    if 'embedding' in model_name.lower():
        max_batch = int((usable_memory * 1e9) / 3000)
        return min(max_batch, 2048)
    
    # LLM inference requires more memory
    return 16
```

**Docker Compose with GPU Support (This Project):**

```yaml
version: '3.8'

# Note: OpenSearch and MongoDB are external services
# Only deploying application components

services:
  embedding-worker:
    build:
      context: .
      dockerfile: Dockerfile.embedding
    environment:
      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - USE_GPU=true
      - BATCH_SIZE=512
      # External Services (configured via .env)
      - OPENSEARCH_HOST=${OPENSEARCH_HOST}
      - OPENSEARCH_PORT=${OPENSEARCH_PORT}
      - MONGODB_URI=${MONGODB_URI}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: python -m src.workers.embedding_worker
    
  llm-inference:
    image: vllm/vllm-openai:latest
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_TENSOR_PARALLEL_SIZE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0']
              capabilities: [gpu]
    command: --model mistralai/Mistral-7B-Instruct-v0.2 --quantization awq --dtype float16

# No volumes needed - OpenSearch is external
# Connection configured via environment variables
```

**Environment Variables (.env file):**

```bash
# External Services
OPENSEARCH_HOST=https://opensearch.colav.co
OPENSEARCH_PORT=9200
OPENSEARCH_USER=marie_agent
OPENSEARCH_PASSWORD=<secret>

MONGODB_URI=mongodb://user:pass@mongodb.colav.co:27017/kahi?authSource=admin

# GPU Configuration
CUDA_VISIBLE_DEVICES=0,1,2
USE_GPU=true

# OpenSearch Index Configuration
INDEX_PREFIX=impactu_marie_agent  # All indices will be named {prefix}_works, {prefix}_person, etc.

# Embedding Model Configuration (Parametrizable)
EMBEDDING_MODEL=BAAI/bge-m3  # Default: Best for academic/multilingual content
# Why BGE-M3?
# - 8K context length (full papers without chunking)
# - Top multilingual performance (outperforms OpenAI)
# - Hybrid retrieval (dense + sparse + colbert)
# - Optimized for Spanish/English scientific literature
# - 7M+ downloads/month, production-proven

# Alternatives:
# EMBEDDING_MODEL=intfloat/multilingual-e5-large  # Similar quality, 512 token limit
# EMBEDDING_MODEL=intfloat/multilingual-e5-base   # Faster, lower resource, 512 token limit
# EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5  # English-focused, 8K context

EMBEDDING_DIMENSION=1024  # BGE-M3 dimension (768 for e5-base, 384 for e5-small)
EMBEDDING_MAX_LENGTH=8192  # BGE-M3 supports up to 8K tokens
EMBEDDING_BATCH_SIZE=256   # Adjust based on GPU memory (256 for BGE-M3)

# LLM Configuration
LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

**Performance Benchmarks:**

> **Note:** K-NN search benchmarks reflect the already deployed OpenSearch cluster with GPU support.

| Operation | CPU (32 cores) | RTX 3090 | A100 (40GB) | Speedup |
|-----------|----------------|----------|-------------|---------|
| Embedding (1K docs) | 12.0s | 0.8s | 0.4s | 15-30x |
| Embedding (10K docs) | 120.0s | 7.5s | 3.8s | 16-32x |
| K-NN Search (1M vectors)* | 450ms | 25ms | 15ms | 18-30x |
| LLM Inference (batch=1) | 8.5s | 0.6s | 0.3s | 14-28x |
| Full Pipeline (RAG query)* | 9.2s | 0.7s | 0.4s | 13-23x |

**\* Powered by external OpenSearch cluster with GPU K-NN**

---

## 8. Monitoring & Maintenance

### 8.1 Key Metrics

**Indexing:**
- Documents indexed per hour
- Embedding generation time
- Index size growth

**Search:**
- Query latency (p50, p95, p99)
- Result relevance scores
- Cache hit rate

**System:**
- OpenSearch cluster health
- CPU/RAM/Disk usage
- MongoDB query performance
- **GPU metrics:**
  - GPU utilization (target: >80%)
  - GPU memory usage
  - GPU temperature (<85Â°C)
  - Power consumption
  - Batch processing throughput

### 8.2 Maintenance Tasks

**Daily:**
- Incremental indexing of updated documents
- Check cluster health (including GPU health)
- Review error logs
- Monitor GPU utilization and temperature
- Check CUDA OOM errors

**Weekly:**
- Analyze search quality metrics
- Update synonyms and stop words
- Backup indices
- Review GPU performance trends
- Check for CUDA driver updates

**Monthly:**
- Full re-indexing validation
- Performance optimization
- Model evaluation
- GPU memory profiling
- Benchmark against CPU baseline

**GPU Health Monitoring:**

```python
import pynvml
from prometheus_client import Gauge

# Initialize NVML
pynvml.nvmlInit()

# Prometheus metrics
gpu_utilization = Gauge('gpu_utilization_percent', 'GPU utilization', ['device'])
gpu_memory_used = Gauge('gpu_memory_used_bytes', 'GPU memory used', ['device'])
gpu_temperature = Gauge('gpu_temperature_celsius', 'GPU temperature', ['device'])
gpu_power_usage = Gauge('gpu_power_usage_watts', 'GPU power usage', ['device'])

def monitor_gpus():
    """Monitor all available GPUs and export metrics."""
    device_count = pynvml.nvmlDeviceGetCount()
    
    for i in range(device_count):
        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
        
        # Get metrics
        util = pynvml.nvmlDeviceGetUtilizationRates(handle)
        memory = pynvml.nvmlDeviceGetMemoryInfo(handle)
        temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)
        power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # mW to W
        
        # Update Prometheus metrics
        gpu_utilization.labels(device=f'gpu{i}').set(util.gpu)
        gpu_memory_used.labels(device=f'gpu{i}').set(memory.used)
        gpu_temperature.labels(device=f'gpu{i}').set(temp)
        gpu_power_usage.labels(device=f'gpu{i}').set(power)
        
        # Alert if temperature too high
        if temp > 85:
            logger.warning(f"GPU {i} temperature high: {temp}Â°C")
```

**Grafana Dashboard Queries:**

```promql
# GPU Utilization
avg(gpu_utilization_percent) by (device)

# GPU Memory Usage
gpu_memory_used_bytes / gpu_memory_total_bytes * 100

# Embedding Throughput (samples/sec)
rate(embeddings_generated_total[5m])

# K-NN Search Latency (p95)
histogram_quantile(0.95, rate(knn_search_duration_seconds_bucket[5m]))

# LLM Tokens per Second
rate(llm_tokens_generated_total[5m])
```

---

## 9. Appendix

### 9.1 Sample Queries

```python
# Example 1: Simple semantic search
query = "machine learning applications"
results = hybrid_search(query, filters={"year": {"gte": 2020}}, top_k=10)

# Example 2: Author search
query = "MarÃ­a GarcÃ­a publications"
person_results = search_person_index(query)
author_id = person_results[0]['id']
works_results = filter_works_by_author(author_id)

# Example 3: Institution analysis
affiliation_id = "universidad-nacional"
works = get_works_by_affiliation(affiliation_id, year=2023)
metrics = aggregate_metrics(works)
```

### 9.2 Configuration Files

See `configs/opensearch_indices.yaml` for detailed index configurations.
See `configs/rag_pipeline.yaml` for ETL pipeline parameters.

---

## 10. References

**Core Technologies:**
- OpenSearch K-NN Documentation: https://opensearch.org/docs/latest/search-plugins/knn/
- OpenSearch K-NN with FAISS GPU: https://opensearch.org/docs/latest/search-plugins/knn/knn-index/
- Sentence Transformers: Multilingual Models: https://www.sbert.net/docs/pretrained_models.html
- MongoDB Best Practices for Large Collections: https://www.mongodb.com/docs/manual/core/data-modeling-introduction/

**GPU Acceleration:**
- NVIDIA CUDA Toolkit: https://developer.nvidia.com/cuda-toolkit
- PyTorch CUDA Installation: https://pytorch.org/get-started/locally/
- FAISS GPU Documentation: https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU
- vLLM GPU Inference: https://docs.vllm.ai/en/latest/
- TensorRT-LLM: https://github.com/NVIDIA/TensorRT-LLM
- NVIDIA DCGM (Data Center GPU Manager): https://developer.nvidia.com/dcgm

**RAG & Search:**
- Hybrid Search Strategies (RRF, Linear Combination): https://www.elastic.co/blog/improving-information-retrieval-elastic-stack-hybrid
- RAG Architecture Patterns: https://python.langchain.com/docs/use_cases/question_answering/
- Semantic Search with Embeddings: https://www.sbert.net/examples/applications/semantic-search/README.html

**Performance & Optimization:**
- Mixed Precision Training (FP16): https://pytorch.org/docs/stable/amp.html
- NVIDIA Nsight Systems: https://developer.nvidia.com/nsight-systems
- Ray for Distributed GPU Workloads: https://docs.ray.io/en/latest/ray-core/walkthrough.html
- Quantization Techniques: https://huggingface.co/docs/transformers/main_classes/quantization

---

**Document Version:** 2.0
**Last Updated:** January 7, 2026
**Status:** Production-Ready with GPU Support
